{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning MNIST Swarm Deployment\n",
    "\n",
    "This notebook demonstrates how to deploy a federated learning swarm for MNIST classification on the Manta platform.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Setting up the connection** to the Manta platform\n",
    "2. **Defining modules and federated learning swarm** with workers, aggregator, and scheduler\n",
    "3. **Deploying the swarm** to a cluster\n",
    "4. **Monitoring results** from training and evaluation\n",
    "5. **Streaming logs** for debugging and tracking progress\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have completed the following steps:\n",
    "\n",
    "### 1. Create an Account\n",
    "- Visit [dashboard.manta-tech.io](https://dashboard.manta-tech.io) and create an account\n",
    "- Create a new cluster and start it\n",
    "\n",
    "### 2. Install Manta SDK\n",
    "Install the Manta SDK in your Python environment:\n",
    "```bash\n",
    "pip install manta-sdk\n",
    "```\n",
    "\n",
    "### 3. Partition the Dataset\n",
    "Run the data preparation script to partition the MNIST dataset for multiple nodes:\n",
    "```bash\n",
    "python prepare_data.py -n <number_of_nodes>\n",
    "```\n",
    "This will create partitioned MNIST data in `temp/partitioned/node_0/`, `temp/partitioned/node_1/`, etc.\n",
    "\n",
    "### 4. Install and Configure Manta Nodes\n",
    "Install manta-node on each device that will participate in training:\n",
    "```bash\n",
    "pip install manta-node\n",
    "```\n",
    "\n",
    "Download node configuration from the Manta dashboard (see \"Configure New Node\" button on your cluster page) and save it in `~/.manta/nodes/<node_name>.toml`\n",
    "\n",
    "**Important**: Configure each node's dataset path to point to its partition:\n",
    "- Node 0: dataset path = `/path/to/temp/partitioned/node_0/mnist.npz`\n",
    "- Node 1: dataset path = `/path/to/temp/partitioned/node_1/mnist.npz`\n",
    "- etc.\n",
    "\n",
    "### 5. Start Manta Nodes\n",
    "Launch each Manta node with its configuration:\n",
    "```bash\n",
    "manta node start <node_name>\n",
    "```\n",
    "\n",
    "Verify nodes are connected by checking your cluster dashboard or running:\n",
    "```bash\n",
    "manta node status\n",
    "```\n",
    "\n",
    "### 6. Docker Image\n",
    "Ensure the Docker image `manta_light:pytorch` is available on your nodes, or modify the `image` variable in this notebook to use a different PyTorch-compatible image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Configure Authentication\n",
    "\n",
    "First, import the necessary libraries and configure your authentication credentials.\n",
    "\n",
    "**Replace the credentials below with your actual account credentials from dashboard.manta-tech.io**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 15:57:11,479 - manta.clients.cluster_management_client.ClusterManagementClient - WARNING - Client for insecure://localhost:50051 was not explicitly disconnected. Attempting synchronous cleanup in __del__. Active calls: 0 (traces.py:473)\n",
      "2025-11-05 15:57:11,482 - manta.clients.module_management_client.ModuleManagementClient - WARNING - Client for insecure://localhost:50051 was not explicitly disconnected. Attempting synchronous cleanup in __del__. Active calls: 0 (traces.py:473)\n",
      "2025-11-05 15:57:11,485 - manta.clients.swarm_management_client.SwarmManagementClient - WARNING - Client for insecure://localhost:50051 was not explicitly disconnected. Attempting synchronous cleanup in __del__. Active calls: 0 (traces.py:473)\n",
      "2025-11-05 15:57:11,491 - manta.clients.user_management_client.UserManagementClient - WARNING - Client for insecure://localhost:50051 was not explicitly disconnected. Attempting synchronous cleanup in __del__. Active calls: 0 (traces.py:473)\n"
     ]
    }
   ],
   "source": [
    "from manta.apis.async_user_api import AsyncUserAPI\n",
    "from manta.common.conversions import bytes_to_dict\n",
    "from pathlib import Path\n",
    "\n",
    "from manta import Module, Task, Swarm\n",
    "from manta.light.utils import numpy_to_bytes\n",
    "\n",
    "from modules.worker.model import MLP\n",
    "\n",
    "# Replace with your credentials from dashboard.manta-tech.io\n",
    "USERNAME = \"admin@manta-tech.io\"\n",
    "PASSWORD = \"admin\"\n",
    "\n",
    "api = await AsyncUserAPI.sign_in(\n",
    "    USERNAME,\n",
    "    PASSWORD,\n",
    "    # host=\"api.manta-tech.io\",\n",
    "    # port=443,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Connect to Manta Platform and Find Active Cluster\n",
    "\n",
    "This section establishes a connection to the Manta platform and locates an active cluster for deployment:\n",
    "\n",
    "1. **Initialize UserAPI**: Creates a connection to the Manta manager service\n",
    "2. **Check availability**: Verifies the connection is working\n",
    "3. **Find active cluster**: Searches for a running cluster to deploy the swarm\n",
    "\n",
    "The cluster API will be used for all subsequent operations including swarm deployment, monitoring, and log streaming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserAPI availability: True\n",
      "\n",
      "Searching for active cluster...\n",
      "===================Active Cluster Found===================\n",
      "Cluster ID: 9bb2a9711ea04cffb2cc96a1d0ceeb99\n",
      "Cluster Name: test\n",
      "Status: RUNNING\n"
     ]
    }
   ],
   "source": [
    "availability_message = await api.is_available()\n",
    "print(f\"UserAPI availability: {availability_message}\")\n",
    "\n",
    "# Find an active (RUNNING) cluster\n",
    "print(\"\\nSearching for active cluster...\")\n",
    "async for cluster in api.stream_clusters():\n",
    "    # Status 1 = RUNNING, 0 = CREATED, 2 = INACTIVE\n",
    "    if cluster.status == 1:\n",
    "        print(\"===================Active Cluster Found===================\")\n",
    "        print(f\"Cluster ID: {cluster.cluster_id}\")\n",
    "        print(f\"Cluster Name: {cluster.name}\")\n",
    "        print(\"Status: RUNNING\")\n",
    "        active_cluster_id = cluster.cluster_id\n",
    "        break\n",
    "else:\n",
    "    print(\"No running cluster found. Please start a cluster from the dashboard.\")\n",
    "    raise RuntimeError(\"No active cluster available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Federated Learning Swarm\n",
    "\n",
    "The `FLSwarm` class defines the complete federated learning workflow with four main components:\n",
    "\n",
    "### Task Components:\n",
    "\n",
    "1. **Aggregator Task**: \n",
    "   - Combines model weights from workers using federated averaging\n",
    "   - Runs on any available node (`method=\"any\"`)\n",
    "   - Limited to 1 instance (`maximum=1`)\n",
    "\n",
    "2. **Worker Train Task**: \n",
    "   - Trains local models on distributed MNIST data\n",
    "   - Runs on all available nodes with data (`method=\"all\"`)\n",
    "   - Unlimited instances (`maximum=-1`)\n",
    "   - Requires MNIST dataset\n",
    "\n",
    "3. **Scheduler Task**: \n",
    "   - Coordinates training rounds and checks convergence\n",
    "   - Decides whether to continue training or stop\n",
    "   - Runs on any available node\n",
    "\n",
    "4. **Worker Test Task**: \n",
    "   - Evaluates the global model on test data\n",
    "   - Runs on all nodes with test data\n",
    "   - Provides validation metrics\n",
    "\n",
    "### Execution Flow:\n",
    "The `execute()` method defines the task graph:\n",
    "```\n",
    "Worker â†’ Aggregator â†’ Test â†’ Scheduler â†’ (loop back to Worker or END)\n",
    "```\n",
    "\n",
    "### Configuration:\n",
    "- **Hyperparameters**: Learning rate, batch size, optimizer settings\n",
    "- **Global Model**: Initial model weights shared across all workers\n",
    "- **Docker Image**: Specifies the container image with manta-light and PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Define Modules\n",
    "\n",
    "First, let's define all the modules that will be used in our federated learning swarm. This separation makes it easier to understand and modify each component independently.\n",
    "\n",
    "### Module Definitions\n",
    "\n",
    "Each module represents a specific task in the federated learning workflow:\n",
    "- **Aggregator Module**: Combines model weights from workers using federated averaging\n",
    "- **Worker Module**: Trains local models on distributed MNIST data\n",
    "- **Scheduler Module**: Coordinates training rounds and checks convergence  \n",
    "- **Worker Test Module**: Evaluates the global model on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modules that will be used in the swarm\n",
    "root_path = Path().resolve()\n",
    "# image = \"ghcr.io/mantatech/manta_light_pytorch:latest\"\n",
    "image = \"manta_light:pytorch\"\n",
    "gpu = False  # Set to True to use GPU\n",
    "\n",
    "# Aggregator Module\n",
    "aggregator_module = Module(\n",
    "    root_path / \"modules\" / \"aggregator.py\",\n",
    "    image,\n",
    "    datasets=[],\n",
    ")\n",
    "\n",
    "# Worker Train Module\n",
    "worker_train_module = Module(\n",
    "    root_path / \"modules\" / \"worker\",\n",
    "    image,\n",
    "    datasets=[\"mnist\"],\n",
    ")\n",
    "\n",
    "# Scheduler Module\n",
    "scheduler_module = Module(\n",
    "    root_path / \"modules\" / \"scheduler.py\",\n",
    "    image,\n",
    "    datasets=[],\n",
    ")\n",
    "\n",
    "# Worker Test Module\n",
    "worker_test_module = Module(\n",
    "    root_path / \"modules\" / \"worker_test\",\n",
    "    image,\n",
    "    datasets=[\"mnist\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Define the Federated Learning Swarm\n",
    "\n",
    "Now we'll create the FLSwarm class that uses the modules defined above. This approach separates the module definitions from the swarm logic, making the code more modular and easier to understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLSwarm(Swarm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aggregator_module: Module,\n",
    "        worker_train_module: Module,\n",
    "        scheduler_module: Module,\n",
    "        worker_test_module: Module,\n",
    "        gpu: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store modules\n",
    "        self.aggregator_module = aggregator_module\n",
    "        self.worker_train_module = worker_train_module\n",
    "        self.scheduler_module = scheduler_module\n",
    "        self.worker_test_module = worker_test_module\n",
    "        self.gpu = gpu\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.set_global(\n",
    "            \"hyperparameters\",\n",
    "            {\n",
    "                \"epochs\": 1,\n",
    "                \"batch_size\": 32,\n",
    "                \"loss\": \"CrossEntropyLoss\",\n",
    "                \"loss_params\": {},\n",
    "                \"optimizer\": \"SGD\",\n",
    "                \"optimizer_params\": {\"lr\": 0.01, \"momentum\": 0.9},\n",
    "                \"val_acc_threshold\": 0.99,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Set global model parameters\n",
    "        self.set_global(\"global_model_params\", numpy_to_bytes(MLP().get_weights()))\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Generation of the task graph\n",
    "\n",
    "        +--------+     +------------+     +------+     +-----------+ if has_converged\n",
    "        | Worker | --> | Aggregator | --> | Test | --> | Scheduler | ----------------> END PROGRAM\n",
    "        +--------+     +------------+     +------+     +-----------+\n",
    "            |                                                       | else\n",
    "            +--<<<----------<<<-------------<<<------------<<<------+\n",
    "        \"\"\"\n",
    "        m = Task(\n",
    "            self.worker_train_module,\n",
    "            gpu=self.gpu,\n",
    "        )()\n",
    "        m = Task(\n",
    "            self.aggregator_module,\n",
    "            method=\"any\",\n",
    "            fixed=True,\n",
    "            maximum=1,\n",
    "        )(m)\n",
    "        m = Task(\n",
    "            self.worker_test_module,\n",
    "            gpu=self.gpu,\n",
    "        )(m)\n",
    "        return Task(\n",
    "            self.scheduler_module,\n",
    "            method=\"any\",\n",
    "            fixed=True,\n",
    "            maximum=1,\n",
    "        )(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Learning Swarm created successfully!\n",
      "Using GPU: False\n",
      "Image: manta_light:pytorch\n"
     ]
    }
   ],
   "source": [
    "# Create the swarm instance using the pre-defined modules\n",
    "swarm = FLSwarm(\n",
    "    aggregator_module=aggregator_module,\n",
    "    worker_train_module=worker_train_module,\n",
    "    scheduler_module=scheduler_module,\n",
    "    worker_test_module=worker_test_module,\n",
    "    gpu=gpu,\n",
    ")\n",
    "\n",
    "print(\"Federated Learning Swarm created successfully!\")\n",
    "print(f\"Using GPU: {gpu}\")\n",
    "print(f\"Image: {image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Deploy the Swarm to the Cluster\n",
    "\n",
    "This section deploys the federated learning swarm to the active cluster:\n",
    "\n",
    "### Deployment Process:\n",
    "1. **Create Swarm Instance**: Instantiate the `FLSwarm` class with desired configuration\n",
    "2. **Deploy to Cluster**: Use `cluster_api.deploy_swarm()` to submit the swarm\n",
    "3. **Get Deployment Overview**: Receive confirmation and metadata about the deployment\n",
    "\n",
    "### Important Deployment Information:\n",
    "- **swarm_id**: Unique identifier for tracking and monitoring the swarm\n",
    "- **status**: Current state (PENDING â†’ RUNNING â†’ COMPLETED)\n",
    "- **task_count**: Number of tasks defined in the swarm\n",
    "- **node_count**: Number of nodes participating in the execution\n",
    "- **iteration**: Current training round (starts at 0)\n",
    "\n",
    "The swarm will automatically start executing once suitable nodes are available and the required Docker images are pulled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deploying swarm...\n",
      "Swarm Deployment Overview: Swarm(swarm_id='bd9db6b91c94473d84752813566754ad', cluster_id='9bb2a9711ea04cffb2cc96a1d0ceeb99', owner_id='496b3f7dda444c9097e747c8d080ae4e', name='Swarm', created_at=datetime.datetime(2025, 11, 5, 15, 28, 12, 159000, tzinfo=datetime.timezone.utc), last_start=None, last_stop=None, status=SwarmStatusEnum.ACTIVE, iteration=0, circular=0, authorization=SwarmAuthorization(swarm_id='bd9db6b91c94473d84752813566754ad', quotas=SwarmQuotas(current_storage_gb=3.814697265625e-06, current_concurrent_tasks=2, updated_at=None), permissions=[SwarmPermission.SWARM_VIEW, SwarmPermission.SWARM_MANAGE, SwarmPermission.SWARM_EXECUTE, SwarmPermission.SWARM_MONITOR, SwarmPermission.SWARM_RESULTS, SwarmPermission.SWARM_READ_ACCESS, SwarmPermission.SWARM_TASK_LIST, SwarmPermission.SWARM_START_ACTION, SwarmPermission.SWARM_STOP_ACTION, SwarmPermission.SWARM_DELETE_ACTION, SwarmPermission.SWARM_RESULTS_READ, SwarmPermission.SWARM_RESULTS_DELETE, SwarmPermission.SWARM_LOGS_READ, SwarmPermission.SWARM_GLOBAL_UPDATE, SwarmPermission.SWARM_GLOBAL_READ]))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDeploying swarm...\")\n",
    "swarm_overview = await api.deploy_swarm(active_cluster_id, swarm)\n",
    "print(f\"Swarm Deployment Overview: {swarm_overview}\")\n",
    "\n",
    "swarm_id = swarm_overview.swarm_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Monitor Training Results\n",
    "\n",
    "This section shows how to monitor the training progress by streaming results from the swarm:\n",
    "\n",
    "### Result Monitoring:\n",
    "- **Stream Results**: Use `cluster_api.stream_results()` to receive real-time updates\n",
    "- **Filter by Tag**: Specify \"metrics\" to get training/validation metrics\n",
    "- **Real-time Updates**: Results arrive as tasks complete each training iteration\n",
    "\n",
    "### Key Metrics Available:\n",
    "- **val_loss**: Validation loss after each training round\n",
    "- **val_acc**: Validation accuracy after each training round\n",
    "- **node_id**: Which node generated the result\n",
    "- **task_id**: Which task (worker/aggregator) produced the metric\n",
    "- **iteration**: Current training round number\n",
    "\n",
    "### Understanding the Output:\n",
    "Each result contains metadata about the task execution and the actual training metrics. The federated learning process will show results from:\n",
    "- **Workers**: Local training metrics from each participating node\n",
    "- **Aggregator**: Global model performance after weight aggregation\n",
    "- **Test Workers**: Validation results on test data\n",
    "\n",
    "ðŸ’¡ **Tip**: The streaming continues until the swarm completes or convergence is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for result in api.stream_results(swarm_id, \"metrics\"):\n",
    "    print(result.tag)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Monitor Execution Logs\n",
    "\n",
    "This section demonstrates how to stream logs from the swarm execution for debugging and progress tracking:\n",
    "\n",
    "### Log Streaming:\n",
    "- **Stream Logs**: Use `cluster_api.stream_logs()` to receive real-time log output\n",
    "- **All Tasks**: Logs from all task types (workers, aggregator, scheduler)\n",
    "- **Debugging Information**: Detailed execution traces and error messages\n",
    "\n",
    "### Log Information Includes:\n",
    "- **node_id**: Which node is executing the task\n",
    "- **task_id**: Specific task instance generating the log\n",
    "- **iteration/circular**: Training round and cycle information\n",
    "- **timestamp**: When the log entry was generated\n",
    "- **severity**: Log level (INFO, WARNING, ERROR, COMPLETED)\n",
    "- **content**: Detailed log messages from the task execution\n",
    "\n",
    "### Log Content Examples:\n",
    "- **Configuration**: Task setup and parameter initialization\n",
    "- **Data Loading**: Dataset access and preparation\n",
    "- **Training Progress**: Model training and validation steps\n",
    "- **Communication**: Inter-task messaging and coordination\n",
    "- **Completion**: Task completion and cleanup\n",
    "\n",
    "### Using Logs for Debugging:\n",
    "- Monitor for ERROR severity messages to identify issues\n",
    "- Track COMPLETED messages to see task progression\n",
    "- Use timestamps to understand execution timing\n",
    "- Check node_id to identify which nodes are having problems\n",
    "\n",
    "ðŸ” **Debugging Tip**: If training seems stuck, check the logs for connection issues, data loading problems, or resource constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for log in api.stream_logs(\"25f532d523864c6884a0e34854b7c707\"):\n",
    "    print(f\"node {log.node_id} task {log.task_id}: {log.severity}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Stream logs for debugging (replace swarm_id with your actual swarm_id)\n",
    "# Uncomment the lines below to stream logs\n",
    "# async for log in api.stream_logs(swarm_id):\n",
    "#     print(f\"[{log.get('severity', 'INFO')}] Node {log.get('node_id', 'unknown')[:8]}: {log.get('content', '')}\")\n",
    "#     print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
